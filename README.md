# ğŸ§ª Local LLM Test Case Generator

A secure, local-first application that generates professional QA test suites from feature requirements using **Ollama** and **Python**. No data leaves your machine.

---

## ğŸ—ï¸ Architecture Diagram

```mermaid
flowchart TB
    Start([ğŸ‘¤ User Opens App]) --> UI[ğŸŒ Streamlit UI Loads]
    UI --> Input{User Enters<br/>Feature Requirement}
    
    Input -->|Example: Login with 2FA| Validate[âœ… Input Validation]
    Validate -->|Valid| Prompt[ğŸ“ Build Structured Prompt]
    Validate -->|Invalid/Empty| Error1[âŒ Show Error Message]
    Error1 --> Input
    
    Prompt --> Backend[âš™ï¸ Backend Generator<br/>generator.py]
    Backend -->|HTTP Request| Ollama[ğŸ¦™ Ollama API<br/>localhost:11434]
    
    subgraph Local_Machine[ğŸ–¥ï¸ Local Machine - No Internet Required]
        Ollama -->|Load Model| Model{ğŸ§  LLM Model<br/>gemma3:1b/llama3.2}
        Model -->|Generate| Response[ğŸ“„ JSON Response]
        Response -->|Validate Schema| Check{Valid JSON?}
        Check -->|No| Retry[ğŸ”„ Retry Logic<br/>Max 2 attempts]
        Retry --> Model
        Check -->|Yes| Return[âœ… Return Test Suite]
    end
    
    Return --> Parse[ğŸ” Parse & Structure Data]
    Parse --> Display[ğŸ¨ Render UI Components]
    
    Display --> Tab1[ğŸ‘ï¸ Visual Cards View]
    Display --> Tab2[ğŸ’» JSON View]
    Display --> Tab3[ğŸ“Š Table + CSV Export]
    
    Tab1 --> End([âœ¨ User Reviews Results])
    Tab2 --> End
    Tab3 --> Download[â¬‡ï¸ Download CSV]
    Download --> End
    
    style Local_Machine fill:#1a1a2e,stroke:#16213e,stroke-width:3px
    style Model fill:#0f3460,stroke:#e94560,stroke-width:2px
    style End fill:#16213e,stroke:#e94560,stroke-width:2px
```

---

## ğŸš€ Features
*   **100% Local**: Uses Ollama API; no cloud tokens or internet required for generation.
*   **Structured Output**: Generates strict JSON for reliable parsing.
*   **Multi-Model Support**: Switch between `gemma3:1b`, `llama3.2`, `mistral`, etc.
*   **Export Ready**: Download test cases as **CSV** or copy as **JSON**.
*   **Premium UI**: Dark mode, distinct priority badges, and tabbed views.

## ğŸ“‚ Project Structure
```bash
Project1-LocalTestCaseGenerator/
â”œâ”€â”€ architecture/        # SOPs and Logic Blueprints
â”œâ”€â”€ backend/             # Core Logic
â”‚   â”œâ”€â”€ generator.py     # Ollama interaction & parsing logic
â”‚   â””â”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ frontend/            # User Interface
â”‚   â””â”€â”€ app.py           # Streamlit Application
â”œâ”€â”€ tools/               # Utility Scripts
â”‚   â””â”€â”€ verify_ollama.py # Connection Test Script
â””â”€â”€ README.md            # Documentation
```

## ğŸ› ï¸ Prerequisites
1.  **Ollama**: [Download Ollama](https://ollama.com/) and run it.
2.  **Pull a Model**:
    ```bash
    ollama pull gemma3:1b
    # OR
    ollama pull llama3.2
    ```
3.  **Python 3.10+**

## âš¡ Quick Start

1.  **Install Dependencies**
    ```bash
    pip install -r backend/requirements.txt
    ```

2.  **Run the App**
    ```bash
    streamlit run frontend/app.py
    ```

3.  **Generate Tests**
    *   Open `http://localhost:8501`.
    *   Select your model in the sidebar.
    *   Type a requirement (e.g., *"Signup page with email verification"*).
    *   Export your results!

---

## ğŸ›¡ï¸ License
MIT
