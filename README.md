# ğŸ§ª Local LLM Test Case Generator

A secure, local-first application that generates professional QA test suites from feature requirements using **Ollama** and **Python**. No data leaves your machine.

---

## ğŸ—ï¸ Architecture Diagram

```mermaid
graph TD
    User([ğŸ‘¤ User]) -->|Enters Requirement| UI[ğŸ’» Streamlit Frontend]
    UI -->|Sends Prompt| Logic[âš™ï¸ Backend Generator]
    Logic -->|API Call (JSON Mode)| Ollama[ğŸ¦™ Ollama Local Server]
    
    subgraph Local Machine
        Ollama -->|Inference| Model{ğŸ§  LLM Model}
        Model -->|gemma3:1b / llama3.2| Ollama
    end
    
    Ollama -->|Structured JSON| Logic
    Logic -->|Parsed Test Cases| UI
    UI -->|Visual Cards / CSV Export| User
```

---

## ğŸš€ Features
*   **100% Local**: Uses Ollama API; no cloud tokens or internet required for generation.
*   **Structured Output**: Generates strict JSON for reliable parsing.
*   **Multi-Model Support**: Switch between `gemma3:1b`, `llama3.2`, `mistral`, etc.
*   **Export Ready**: Download test cases as **CSV** or copy as **JSON**.
*   **Premium UI**: Dark mode, distinct priority badges, and tabbed views.

## ğŸ“‚ Project Structure
```bash
Project1-LocalTestCaseGenerator/
â”œâ”€â”€ architecture/        # SOPs and Logic Blueprints
â”œâ”€â”€ backend/             # Core Logic
â”‚   â”œâ”€â”€ generator.py     # Ollama interaction & parsing logic
â”‚   â””â”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ frontend/            # User Interface
â”‚   â””â”€â”€ app.py           # Streamlit Application
â”œâ”€â”€ tools/               # Utility Scripts
â”‚   â””â”€â”€ verify_ollama.py # Connection Test Script
â””â”€â”€ README.md            # Documentation
```

## ğŸ› ï¸ Prerequisites
1.  **Ollama**: [Download Ollama](https://ollama.com/) and run it.
2.  **Pull a Model**:
    ```bash
    ollama pull gemma3:1b
    # OR
    ollama pull llama3.2
    ```
3.  **Python 3.10+**

## âš¡ Quick Start

1.  **Install Dependencies**
    ```bash
    pip install -r backend/requirements.txt
    ```

2.  **Run the App**
    ```bash
    streamlit run frontend/app.py
    ```

3.  **Generate Tests**
    *   Open `http://localhost:8501`.
    *   Select your model in the sidebar.
    *   Type a requirement (e.g., *"Signup page with email verification"*).
    *   Export your results!

---

## ğŸ›¡ï¸ License
MIT
